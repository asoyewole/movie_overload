{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Import Libraries and Set Up Logging\n",
    "import logging\n",
    "import os\n",
    "import time\n",
    "import gzip\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, log, when, median, to_date, coalesce, concat, lit\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, TimestampType\n",
    "from pyspark.ml.recommendation import ALS\n",
    "from pyspark.ml.evaluation import RegressionEvaluator, RankingEvaluator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Embedding, Flatten, Concatenate, Dense, Dropout\n",
    "import numpy as np\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 12:34:25,625 - INFO - Created Spark session: http://127.0.0.1:4040\n",
      "2025-08-04 12:34:25,625 - INFO - Created Spark session: http://127.0.0.1:4040\n",
      "2025-08-04 12:34:25,628 - INFO - Spark session initialized, logging configured, and TMDb credentials loaded.\n",
      "2025-08-04 12:34:25,628 - INFO - Spark session initialized, logging configured, and TMDb credentials loaded.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    filename='recommendation_pipeline.log',\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    datefmt='%Y-%m-%d %H:%M:%S'\n",
    ")\n",
    "console = logging.StreamHandler()\n",
    "console.setLevel(logging.INFO)\n",
    "console.setFormatter(logging.Formatter(\n",
    "    '%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logging.getLogger().addHandler(console)\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "TMDB_API_KEY = os.getenv('TMDB_API_KEY')\n",
    "TMDB_API_READ_ACCESS_TOKEN = os.getenv('TMDB_API_READ_ACCESS_TOKEN')\n",
    "\n",
    "# Set HADOOP_HOME for Windows\n",
    "os.environ[\"HADOOP_HOME\"] = \"C:\\\\hadoop-3.3.6\"\n",
    "os.environ[\"hadoop.home.dir\"] = \"C:\\\\hadoop-3.3.6\"\n",
    "\n",
    "# Stop any existing Spark sessions\n",
    "spark = SparkSession.getActiveSession()\n",
    "if spark:\n",
    "    spark.stop()\n",
    "    logging.info(\"Stopped existing Spark session\")\n",
    "\n",
    "# Initialize Spark session\n",
    "os.environ[\"PYSPARK_PYTHON\"] = os.path.abspath(\".venv/Scripts/python.exe\")\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CostAwareRecommendation\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.driver.memory\", \"8g\") \\\n",
    "    .config(\"spark.pyspark.python\", os.environ[\"PYSPARK_PYTHON\"]) \\\n",
    "    .config(\"spark.driver.bindAddress\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.host\", \"127.0.0.1\") \\\n",
    "    .config(\"spark.driver.port\", \"25334\") \\\n",
    "    .config(\"spark.ui.showConsoleProgress\", \"false\") \\\n",
    "    .config(\"spark.driver.extraJavaOptions\", \"-Xss4m\") \\\n",
    "    .config(\"spark.executor.extraJavaOptions\", \"-Xss4m\") \\\n",
    "    .getOrCreate()\n",
    "logging.info(f\"Created Spark session: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "logging.info(\n",
    "    \"Spark session initialized, logging configured, and TMDb credentials loaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Fetch Budget Data from TMDb API\n",
    "def fetch_tmdb_budgets(links_df):\n",
    "    try:\n",
    "        # Convert links to Pandas for API calls\n",
    "        links_pd = links_df.select(\"movieId\", \"tmdbId\").toPandas()\n",
    "        links_pd = links_pd[links_pd['tmdbId'].notnull()]\n",
    "\n",
    "        budgets = []\n",
    "        base_url = \"https://api.themoviedb.org/3/movie/\"\n",
    "        headers = {\"Authorization\": f\"Bearer {TMDB_API_READ_ACCESS_TOKEN}\"}\n",
    "\n",
    "        logging.info(\n",
    "            f\"Fetching budgets for {len(links_pd)} movies from TMDb...\")\n",
    "        for idx, row in links_pd.iterrows():\n",
    "            tmdb_id = str(int(row['tmdbId']))\n",
    "            url = f\"{base_url}{tmdb_id}?api_key={TMDB_API_KEY}\"\n",
    "            response = requests.get(url, headers=headers)\n",
    "\n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                budgets.append({\n",
    "                    'movieId': row['movieId'],\n",
    "                    'tmdbId': tmdb_id,\n",
    "                    'budget': data.get('budget', 0),\n",
    "                    'release_date': data.get('release_date', '')\n",
    "                })\n",
    "            else:\n",
    "                logging.warning(\n",
    "                    f\"Failed to fetch budget for tmdbId {tmdb_id}: {response.status_code}\")\n",
    "\n",
    "            # Respect rate limit (50 requests/sec < 0.03 sec/request)\n",
    "            time.sleep(0.03)\n",
    "\n",
    "            if (idx + 1) % 100 == 0:\n",
    "                logging.info(f\"Processed {idx + 1} movies...\")\n",
    "\n",
    "        # Save budgets as CSV\n",
    "        budgets_df = pd.DataFrame(budgets)\n",
    "        budgets_df.to_csv(\"tmdb_budgets.csv\", index=False)\n",
    "\n",
    "        # Convert to Spark DataFrame\n",
    "        budgets_spark = spark.createDataFrame(budgets_df)\n",
    "        logging.info(f\"Fetched and saved {len(budgets)} budgets from TMDb.\")\n",
    "        return budgets_spark\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to fetch TMDb budgets: {str(e)}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:51:16,636 - INFO - Loading budgets from existing CSV...\n",
      "2025-08-04 00:51:22,036 - INFO - Loaded all datasets and budgets.\n"
     ]
    }
   ],
   "source": [
    "def load_data_and_budgets():\n",
    "    try:\n",
    "        # Define schemas\n",
    "        ratings_schema = StructType([\n",
    "            StructField(\"userId\", IntegerType(), False),\n",
    "            StructField(\"movieId\", IntegerType(), False),\n",
    "            StructField(\"rating\", FloatType(), False),\n",
    "            StructField(\"timestamp\", IntegerType(), False)\n",
    "        ])\n",
    "        movies_schema = StructType([\n",
    "            StructField(\"movieId\", IntegerType(), False),\n",
    "            StructField(\"title\", StringType(), False),\n",
    "            StructField(\"genres\", StringType(), True)\n",
    "        ])\n",
    "        links_schema = StructType([\n",
    "            StructField(\"movieId\", IntegerType(), False),\n",
    "            StructField(\"imdbId\", StringType(), True),\n",
    "            StructField(\"tmdbId\", StringType(), True)\n",
    "        ])\n",
    "        title_basics_schema = StructType([\n",
    "            StructField(\"tconst\", StringType(), False),\n",
    "            StructField(\"titleType\", StringType(), True),\n",
    "            StructField(\"primaryTitle\", StringType(), True),\n",
    "            StructField(\"originalTitle\", StringType(), True),\n",
    "            StructField(\"isAdult\", StringType(), True),\n",
    "            StructField(\"startYear\", StringType(), True),\n",
    "            StructField(\"endYear\", StringType(), True),\n",
    "            StructField(\"runtimeMinutes\", StringType(), True),\n",
    "            StructField(\"genres\", StringType(), True)\n",
    "        ])\n",
    "\n",
    "        # Load MovieLens data\n",
    "        ratings_df = spark.read.schema(ratings_schema).csv(\n",
    "            \"ml-25m/ratings.csv\", header=True)\n",
    "        movies_df = spark.read.schema(movies_schema).csv(\n",
    "            \"ml-25m/movies.csv\", header=True)\n",
    "        links_df = spark.read.schema(links_schema).csv(\n",
    "            \"ml-25m/links.csv\", header=True)\n",
    "\n",
    "        # Load IMDb data\n",
    "        basics_df = spark.read.schema(title_basics_schema).option(\n",
    "            \"delimiter\", \"\\t\").csv(\"imdb/title.basics.tsv.gz\", header=True)\n",
    "\n",
    "        # Check if budgets CSV exists\n",
    "        budgets_path = os.path.abspath(\"tmdb_budgets.csv\")\n",
    "        if os.path.exists(budgets_path):\n",
    "            logging.info(\"Loading budgets from existing CSV...\")\n",
    "            budgets_df = spark.read.csv(\n",
    "                budgets_path, header=True, inferSchema=True)\n",
    "        else:\n",
    "            # Fetch TMDb budgets\n",
    "            budgets_df = fetch_tmdb_budgets(links_df)\n",
    "            # Ensure output directory exists\n",
    "            os.makedirs(os.path.dirname(budgets_path), exist_ok=True)\n",
    "            # Save budgets as CSV\n",
    "            budgets_df.write.mode(\"overwrite\").csv(budgets_path)\n",
    "\n",
    "        # Format imdbId to match tconst (add 'tt' prefix)\n",
    "        links_df = links_df.withColumn(\n",
    "            \"imdbId\", concat(lit(\"tt\"), col(\"imdbId\")))\n",
    "\n",
    "        logging.info(\"Loaded all datasets and budgets.\")\n",
    "        return ratings_df, movies_df, links_df, basics_df, budgets_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to load data and budgets: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Check if DataFrames are already defined to avoid re-running\n",
    "if 'ratings_df' not in globals() or 'movies_df' not in globals() or \\\n",
    "   'links_df' not in globals() or 'basics_df' not in globals() or \\\n",
    "   'budgets_df' not in globals():\n",
    "    ratings_df, movies_df, links_df, basics_df, budgets_df = load_data_and_budgets()\n",
    "else:\n",
    "    logging.info(\"DataFrames already loaded, skipping load_data_and_budgets.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:51:25,458 - INFO - Starting preprocess_data at 1754286685.4580612\n",
      "2025-08-04 00:51:25,458 - INFO - Validating input DataFrames...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:51:29,203 - INFO - ratings_df has 33832162 rows\n",
      "2025-08-04 00:51:29,563 - INFO - movies_df has 86537 rows\n",
      "2025-08-04 00:51:29,776 - INFO - links_df has 86537 rows\n",
      "2025-08-04 00:51:38,487 - INFO - basics_df has 11771649 rows\n",
      "2025-08-04 00:51:38,699 - INFO - budgets_df has 85305 rows\n",
      "2025-08-04 00:51:39,594 - INFO - ratings_df schema: StructType([StructField('movieId', IntegerType(), True), StructField('userId', IntegerType(), True), StructField('rating', FloatType(), True), StructField('timestamp', IntegerType(), True), StructField('count', LongType(), False)])\n",
      "2025-08-04 00:52:13,003 - INFO - ratings_df sample: None\n",
      "2025-08-04 00:52:13,004 - INFO - movies_df schema: StructType([StructField('genres', StringType(), True), StructField('movieId', IntegerType(), True), StructField('title', StringType(), True), StructField('budget', DoubleType(), False), StructField('release_date', DateType(), True), StructField('budget_normalized', DoubleType(), True)])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+-----+\n",
      "|movieId|userId|rating| timestamp|count|\n",
      "+-------+------+------+----------+-----+\n",
      "|     28|   175|   5.0| 939073272| 3497|\n",
      "|     28|   221|   4.0| 852284529| 3497|\n",
      "|     28|   224|   3.0|1123310812| 3497|\n",
      "|     28|   298|   4.0|1236927876| 3497|\n",
      "|     28|   347|   3.0| 837372429| 3497|\n",
      "+-------+------+------+----------+-----+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:53:06,361 - INFO - movies_df sample: None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------+--------------------+---------+------------+------------------+\n",
      "|              genres|movieId|               title|   budget|release_date| budget_normalized|\n",
      "+--------------------+-------+--------------------+---------+------------+------------------+\n",
      "|       Drama,Fantasy| 167420|  Master i Margarita|      0.0|  1994-06-06|               0.0|\n",
      "|Adventure,Animati...|   3751|         Chicken Run|    4.5E7|  2000-06-23|17.622173047734595|\n",
      "|Biography,Drama,R...|   5791|               Frida|    1.2E7|  2002-08-29|16.300417207752275|\n",
      "|Action,Adventure,...|  33493|Star Wars: Episod...|   1.13E8|  2005-05-17|18.542898376676614|\n",
      "|Drama,Mystery,Rom...|   3457|     Waking the Dead|8500000.0|  2000-03-24|15.955576721460545|\n",
      "+--------------------+-------+--------------------+---------+------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 00:54:25,178 - INFO - Preprocessed 33677092 ratings and 50888 movies.\n"
     ]
    }
   ],
   "source": [
    "def preprocess_data(ratings_df, movies_df, links_df, basics_df, budgets_df):\n",
    "    try:\n",
    "        # Log start of preprocessing\n",
    "        logging.info(f\"Starting preprocess_data at {time.time()}\")\n",
    "\n",
    "        # Validate input DataFrames\n",
    "        logging.info(\"Validating input DataFrames...\")\n",
    "        for df, name in [(ratings_df, \"ratings_df\"), (movies_df, \"movies_df\"),\n",
    "                         (links_df, \"links_df\"), (basics_df, \"basics_df\"),\n",
    "                         (budgets_df, \"budgets_df\")]:\n",
    "            if df is None:\n",
    "                raise ValueError(f\"{name} is None\")\n",
    "            df_count = df.count()\n",
    "            logging.info(f\"{name} has {df_count} rows\")\n",
    "\n",
    "        # Clean startYear by replacing '\\N' with null in basics_df\n",
    "        basics_df = basics_df.withColumn(\n",
    "            \"startYear\",\n",
    "            when(col(\"startYear\") != \"\\\\N\", col(\"startYear\")).otherwise(None)\n",
    "        )\n",
    "\n",
    "        # Merge datasets\n",
    "        movies_df = (\n",
    "            movies_df\n",
    "            .join(links_df, \"movieId\", \"inner\")\n",
    "            .join(\n",
    "                basics_df.select(\n",
    "                    col(\"tconst\"),\n",
    "                    col(\"primaryTitle\"),\n",
    "                    col(\"startYear\"),\n",
    "                    col(\"genres\").alias(\"basics_genres\")\n",
    "                ),\n",
    "                col(\"imdbId\") == col(\"tconst\"),\n",
    "                \"left\"\n",
    "            )\n",
    "            .join(budgets_df, \"movieId\", \"left\")\n",
    "            .filter(\n",
    "                (col(\"startYear\").isNotNull() & col(\"startYear\").cast(\"int\").between(2000, 2025)) |\n",
    "                (col(\"release_date\").isNotNull() & to_date(\n",
    "                    col(\"release_date\")).cast(\"string\").like(\"20[0-2][0-5]%\"))\n",
    "            )\n",
    "            .select(\n",
    "                col(\"movieId\"),\n",
    "                col(\"primaryTitle\").alias(\"title\"),\n",
    "                coalesce(col(\"basics_genres\"), col(\"genres\")).alias(\"genres\"),\n",
    "                col(\"budget\").cast(\"float\"),\n",
    "                to_date(col(\"release_date\")).alias(\"release_date\")\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # Remove sparse ratings\n",
    "        movie_counts = ratings_df.groupBy(\n",
    "            \"movieId\").count().filter(col(\"count\") >= 10)\n",
    "        ratings_df = ratings_df.join(movie_counts, \"movieId\")\n",
    "\n",
    "        # Clean genres for consistent format\n",
    "        movies_df = movies_df.withColumn(\n",
    "            \"genres\",\n",
    "            when(col(\"genres\").isNotNull(), col(\n",
    "                \"genres\")).otherwise(lit(\"Unknown\"))\n",
    "        )\n",
    "\n",
    "        # Impute missing budgets\n",
    "        genre_medians = movies_df.filter(col(\"budget\") > 0).groupBy(\n",
    "            \"genres\").agg(median(\"budget\").alias(\"median_budget\"))\n",
    "        movies_df = movies_df.join(genre_medians, \"genres\", \"left\") \\\n",
    "            .withColumn(\"budget\", coalesce(col(\"budget\"), col(\"median_budget\"), lit(1))) \\\n",
    "            .drop(\"median_budget\")\n",
    "\n",
    "        # Normalize budgets\n",
    "        movies_df = movies_df.withColumn(\n",
    "            \"budget_normalized\",\n",
    "            log(when(col(\"budget\") > 0, col(\"budget\")).otherwise(1))\n",
    "        )\n",
    "\n",
    "        # Save preprocessed data\n",
    "        try:\n",
    "            # Use absolute paths for Parquet files\n",
    "            ratings_path = os.path.abspath(\"preprocessed_ratings\")\n",
    "            movies_path = os.path.abspath(\"preprocessed_movies\")\n",
    "            os.makedirs(ratings_path, exist_ok=True)\n",
    "            os.makedirs(movies_path, exist_ok=True)\n",
    "\n",
    "            # Validate DataFrames before writing\n",
    "            logging.info(f\"ratings_df schema: {ratings_df.schema}\")\n",
    "            logging.info(f\"ratings_df sample: {ratings_df.show(5)}\")\n",
    "            logging.info(f\"movies_df schema: {movies_df.schema}\")\n",
    "            logging.info(f\"movies_df sample: {movies_df.show(5)}\")\n",
    "\n",
    "            # ratings_df.write.mode(\"overwrite\").parquet(ratings_path)\n",
    "            # movies_df.write.mode(\"overwrite\").parquet(movies_path)\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save preprocessed data: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "        logging.info(\n",
    "            f\"Preprocessed {ratings_df.count()} ratings and {movies_df.count()} movies.\")\n",
    "        return ratings_df, movies_df\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to preprocess data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Call the function\n",
    "ratings_df, movies_df = preprocess_data(\n",
    "    ratings_df, movies_df, links_df, basics_df, budgets_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: 26940320 rows\n",
      "Test set size: 6736772 rows\n"
     ]
    }
   ],
   "source": [
    " # Split ratings into train (80%) and test (20%)\n",
    "train_df, test_df = ratings_df.randomSplit([0.8, 0.2], seed=42)\n",
    "train_count = train_df.count()\n",
    "test_count = test_df.count()\n",
    "print(f\"Train set size: {train_count} rows\")\n",
    "print(f\"Test set size: {test_count} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 18:35:52,529 - INFO - ALS model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Build Matrix Factorization Model (ALS with Budget Weights)\n",
    "def build_als_model(train_df, movies_df):\n",
    "    try:\n",
    "        # Merge budget with ratings\n",
    "        ratings_with_budget = train_df.join(\n",
    "            movies_df.select(\"movieId\", \"budget_normalized\"), \"movieId\")\n",
    "\n",
    "        # Train ALS model\n",
    "        als = ALS(\n",
    "            maxIter=10,\n",
    "            regParam=0.1,\n",
    "            userCol=\"userId\",\n",
    "            itemCol=\"movieId\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True\n",
    "        )\n",
    "        model = als.fit(ratings_with_budget)\n",
    "\n",
    "        # Save model\n",
    "        model.write().overwrite().save(\"als_model\")\n",
    "        logging.info(\"ALS model trained and saved.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to build ALS model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "als_model = build_als_model(train_df, movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-24 19:18:20,133 - INFO - Closing down clientserver connection\n",
      "2025-07-24 19:18:20,234 - INFO - Best maxIter: 15\n",
      "2025-07-24 19:18:20,281 - INFO - Best regParam: 0.1\n",
      "2025-07-24 19:18:22,585 - INFO - Best ALS model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "import logging\n",
    "\n",
    "\n",
    "def build_als_model_with_cv(train_df, movies_df, num_folds=3):\n",
    "    try:\n",
    "        # Merge budget with ratings\n",
    "        ratings_with_budget = train_df.join(\n",
    "            movies_df.select(\"movieId\", \"budget_normalized\"), \"movieId\")\n",
    "\n",
    "        # Define ALS model\n",
    "        als = ALS(\n",
    "            userCol=\"userId\",\n",
    "            itemCol=\"movieId\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True\n",
    "        )\n",
    "\n",
    "        # Define evaluator (RMSE for regression)\n",
    "        evaluator = RegressionEvaluator(\n",
    "            metricName=\"rmse\",\n",
    "            labelCol=\"rating\",\n",
    "            predictionCol=\"prediction\"\n",
    "        )\n",
    "\n",
    "        # Define parameter grid for hyperparameter tuning\n",
    "        param_grid = ParamGridBuilder() \\\n",
    "            .addGrid(als.maxIter, [5, 10, 15]) \\\n",
    "            .addGrid(als.regParam, [0.01, 0.1, 0.5]) \\\n",
    "            .build()\n",
    "\n",
    "        # Set up CrossValidator\n",
    "        crossval = CrossValidator(\n",
    "            estimator=als,\n",
    "            estimatorParamMaps=param_grid,\n",
    "            evaluator=evaluator,\n",
    "            numFolds=num_folds,\n",
    "            collectSubModels=True\n",
    "        )\n",
    "\n",
    "        # Train model with cross-validation\n",
    "        cv_model = crossval.fit(ratings_with_budget)\n",
    "\n",
    "        # Get the best model\n",
    "        best_model = cv_model.bestModel\n",
    "\n",
    "        # Log best parameters\n",
    "        logging.info(\n",
    "            f\"Best maxIter: {best_model._java_obj.parent().getMaxIter()}\")\n",
    "        logging.info(\n",
    "            f\"Best regParam: {best_model._java_obj.parent().getRegParam()}\")\n",
    "\n",
    "        # Save best model\n",
    "        best_model.write().overwrite().save(\"als_model_cv\")\n",
    "        logging.info(\"Best ALS model trained and saved.\")\n",
    "        return best_model\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to build ALS model with CV: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Call the function\n",
    "als_model_cv = build_als_model_with_cv(train_df, movies_df, num_folds=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1796s\u001b[0m 51ms/step - loss: 0.0312 - val_loss: 0.0375\n",
      "Epoch 2/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1776s\u001b[0m 50ms/step - loss: 0.0259 - val_loss: 0.0384\n",
      "Epoch 3/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1709s\u001b[0m 48ms/step - loss: 0.0241 - val_loss: 0.0401\n",
      "Epoch 4/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1613s\u001b[0m 46ms/step - loss: 0.0226 - val_loss: 0.0404\n",
      "Epoch 5/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2162s\u001b[0m 61ms/step - loss: 0.0214 - val_loss: 0.0419\n",
      "Epoch 6/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2062s\u001b[0m 58ms/step - loss: 0.0205 - val_loss: 0.0425\n",
      "Epoch 7/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1652s\u001b[0m 47ms/step - loss: 0.0197 - val_loss: 0.0434\n",
      "Epoch 8/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1666s\u001b[0m 47ms/step - loss: 0.0191 - val_loss: 0.0433\n",
      "Epoch 9/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1670s\u001b[0m 47ms/step - loss: 0.0185 - val_loss: 0.0440\n",
      "Epoch 10/10\n",
      "\u001b[1m35344/35344\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1666s\u001b[0m 47ms/step - loss: 0.0181 - val_loss: 0.0449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 00:30:50,706 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n",
      "2025-07-25 00:30:51,189 - INFO - NCF model trained and saved.\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Build Neural Collaborative Filtering (NCF) Model\n",
    "def build_ncf_model(train_df, movies_df):\n",
    "    try:\n",
    "        # Convert to Pandas for TensorFlow\n",
    "        ratings_pd = train_df.join(movies_df.select(\n",
    "            \"movieId\", \"budget_normalized\"), \"movieId\").toPandas()\n",
    "\n",
    "        # Prepare data\n",
    "        user_ids = ratings_pd[\"userId\"].unique()\n",
    "        movie_ids = ratings_pd[\"movieId\"].unique()\n",
    "        user_map = {id: idx for idx, id in enumerate(user_ids)}\n",
    "        movie_map = {id: idx for idx, id in enumerate(movie_ids)}\n",
    "        ratings_pd[\"user_idx\"] = ratings_pd[\"userId\"].map(user_map)\n",
    "        ratings_pd[\"movie_idx\"] = ratings_pd[\"movieId\"].map(movie_map)\n",
    "        max_budget = ratings_pd[\"budget_normalized\"].max()\n",
    "\n",
    "        # Define NCF model\n",
    "        user_input = Input(shape=(1,), name=\"user\")\n",
    "        movie_input = Input(shape=(1,), name=\"movie\")\n",
    "        budget_input = Input(shape=(1,), name=\"budget\")\n",
    "\n",
    "        user_embedding = Embedding(len(user_ids), 50)(user_input)\n",
    "        movie_embedding = Embedding(len(movie_ids), 50)(movie_input)\n",
    "        budget_dense = Dense(10, activation=\"relu\")(budget_input)\n",
    "\n",
    "        user_flat = Flatten()(user_embedding)\n",
    "        movie_flat = Flatten()(movie_embedding)\n",
    "        concat = Concatenate()([user_flat, movie_flat, budget_dense])\n",
    "\n",
    "        dense = Dense(128, activation=\"relu\")(concat)\n",
    "        dense = Dropout(0.2)(dense)\n",
    "        output = Dense(1, activation=\"sigmoid\")(dense)\n",
    "\n",
    "        model = Model(inputs=[user_input, movie_input,\n",
    "                      budget_input], outputs=output)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "\n",
    "        # Train model\n",
    "        model.fit(\n",
    "            [ratings_pd[\"user_idx\"], ratings_pd[\"movie_idx\"],\n",
    "                ratings_pd[\"budget_normalized\"] / max_budget],\n",
    "            ratings_pd[\"rating\"] / 5.0,\n",
    "            epochs=10,\n",
    "            batch_size=256,\n",
    "            validation_split=0.2\n",
    "        )\n",
    "\n",
    "        # Save model\n",
    "        model.save(\"ncf_model.h5\")\n",
    "        logging.info(\"NCF model trained and saved.\")\n",
    "        return model, user_map, movie_map\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to build NCF model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "ncf_model, user_map, movie_map = build_ncf_model(train_df, movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 00:30:51,242 - INFO - Starting ALS baseline model training...\n",
      "2025-07-25 00:31:30,107 - INFO - Input data size: 26940320 ratings\n",
      "2025-07-25 00:31:30,184 - INFO - Fitting ALS baseline model...\n",
      "2025-07-25 00:33:35,221 - WARNING - Model path als_baseline_model already exists, overwriting...\n",
      "2025-07-25 00:33:36,534 - INFO - ALS baseline model saved to als_baseline_model\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def build_als_baseline_model(train_df, save_path=\"als_baseline_model\"):\n",
    "    \"\"\"\n",
    "    Train and save a baseline ALS model without budget information.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Spark DataFrame with userId, movieId, rating columns\n",
    "        save_path: Path to save the trained model\n",
    "    Returns:\n",
    "        Trained ALS model\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting ALS baseline model training...\")\n",
    "\n",
    "        # Select relevant columns\n",
    "        ratings_baseline = train_df.select(\"userId\", \"movieId\", \"rating\")\n",
    "        logging.info(f\"Input data size: {ratings_baseline.count()} ratings\")\n",
    "\n",
    "        # Define ALS model\n",
    "        als_baseline = ALS(\n",
    "            maxIter=10,\n",
    "            regParam=0.1,\n",
    "            userCol=\"userId\",\n",
    "            itemCol=\"movieId\",\n",
    "            ratingCol=\"rating\",\n",
    "            coldStartStrategy=\"drop\",\n",
    "            nonnegative=True\n",
    "        )\n",
    "\n",
    "        # Train model\n",
    "        logging.info(\"Fitting ALS baseline model...\")\n",
    "        als_baseline_model = als_baseline.fit(ratings_baseline)\n",
    "\n",
    "        # Save model\n",
    "        if os.path.exists(save_path):\n",
    "            logging.warning(\n",
    "                f\"Model path {save_path} already exists, overwriting...\")\n",
    "        als_baseline_model.write().overwrite().save(save_path)\n",
    "        logging.info(f\"ALS baseline model saved to {save_path}\")\n",
    "\n",
    "        return als_baseline_model\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to build ALS baseline model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Call the function\n",
    "als_baseline_model = build_als_baseline_model(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 00:33:36,697 - INFO - Starting NCF baseline model training...\n",
      "2025-07-25 00:38:36,269 - INFO - Input data size: 26940320 ratings\n",
      "2025-07-25 00:38:38,017 - INFO - Number of unique users: 328892, movies: 32021\n",
      "2025-07-25 00:38:38,887 - INFO - NCF baseline model architecture defined\n",
      "2025-07-25 00:38:38,887 - INFO - Fitting NCF baseline model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5690s\u001b[0m 68ms/step - loss: 0.0317 - val_loss: 0.0400\n",
      "Epoch 2/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5606s\u001b[0m 67ms/step - loss: 0.0268 - val_loss: 0.0431\n",
      "Epoch 3/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5787s\u001b[0m 69ms/step - loss: 0.0249 - val_loss: 0.0437\n",
      "Epoch 4/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5718s\u001b[0m 68ms/step - loss: 0.0235 - val_loss: 0.0472\n",
      "Epoch 5/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5699s\u001b[0m 68ms/step - loss: 0.0225 - val_loss: 0.0462\n",
      "Epoch 6/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6550s\u001b[0m 78ms/step - loss: 0.0217 - val_loss: 0.0468\n",
      "Epoch 7/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6503s\u001b[0m 77ms/step - loss: 0.0211 - val_loss: 0.0484\n",
      "Epoch 8/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5655s\u001b[0m 67ms/step - loss: 0.0206 - val_loss: 0.0486\n",
      "Epoch 9/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5601s\u001b[0m 67ms/step - loss: 0.0201 - val_loss: 0.0493\n",
      "Epoch 10/10\n",
      "\u001b[1m84189/84189\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5592s\u001b[0m 66ms/step - loss: 0.0198 - val_loss: 0.0493\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def build_ncf_baseline_model(train_df, save_path=\"ncf_baseline_model.h5\"):\n",
    "    \"\"\"\n",
    "    Train and save a baseline NCF model without budget information.\n",
    "    \n",
    "    Args:\n",
    "        train_df: Spark DataFrame with userId, movieId, rating columns\n",
    "        save_path: Path to save the trained model\n",
    "    Returns:\n",
    "        Trained NCF model, user_map, movie_map\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting NCF baseline model training...\")\n",
    "\n",
    "        # Convert to Pandas\n",
    "        ratings_pd = train_df.toPandas()\n",
    "        logging.info(f\"Input data size: {len(ratings_pd)} ratings\")\n",
    "\n",
    "        # Prepare data\n",
    "        user_ids = ratings_pd[\"userId\"].unique()\n",
    "        movie_ids = ratings_pd[\"movieId\"].unique()\n",
    "        user_map = {id: idx for idx, id in enumerate(user_ids)}\n",
    "        movie_map = {id: idx for idx, id in enumerate(movie_ids)}\n",
    "        ratings_pd[\"user_idx\"] = ratings_pd[\"userId\"].map(user_map)\n",
    "        ratings_pd[\"movie_idx\"] = ratings_pd[\"movieId\"].map(movie_map)\n",
    "        logging.info(\n",
    "            f\"Number of unique users: {len(user_ids)}, movies: {len(movie_ids)}\")\n",
    "\n",
    "        # Define NCF model\n",
    "        user_input = Input(shape=(1,), name=\"user\")\n",
    "        movie_input = Input(shape=(1,), name=\"movie\")\n",
    "        user_embedding = Embedding(len(user_ids), 50)(user_input)\n",
    "        movie_embedding = Embedding(len(movie_ids), 50)(movie_input)\n",
    "        user_flat = Flatten()(user_embedding)\n",
    "        movie_flat = Flatten()(movie_embedding)\n",
    "        concat = Concatenate()([user_flat, movie_flat])\n",
    "        dense = Dense(128, activation=\"relu\")(concat)\n",
    "        dense = Dropout(0.2)(dense)\n",
    "        output = Dense(1, activation=\"sigmoid\")(dense)\n",
    "\n",
    "        model = Model(inputs=[user_input, movie_input], outputs=output)\n",
    "        model.compile(optimizer=\"adam\", loss=\"mse\")\n",
    "        logging.info(\"NCF baseline model architecture defined\")\n",
    "\n",
    "        # Train model\n",
    "        logging.info(\"Fitting NCF baseline model...\")\n",
    "        model.fit(\n",
    "            [ratings_pd[\"user_idx\"], ratings_pd[\"movie_idx\"]],\n",
    "            ratings_pd[\"rating\"] / 5.0,\n",
    "            epochs=10,\n",
    "            batch_size=256,\n",
    "            validation_split=0.2,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "\n",
    "        return model, user_map, movie_map\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to build NCF baseline model: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Call the function\n",
    "ncf_baseline_model, user_map_baseline, movie_map_baseline = build_ncf_baseline_model(\n",
    "    train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 17:09:00,969 - WARNING - You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 17:09:01,649 - INFO - NCF baseline model saved to ncf_baseline_model.h5\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Save model\n",
    "if os.path.exists(\"ncf_baseline_model.h5\"):\n",
    "    logging.warning(\n",
    "        \"Model path ncf_baseline_model.h5 already exists, overwriting...\")\n",
    "ncf_baseline_model.save(\"ncf_baseline_model.h5\")\n",
    "logging.info(\"NCF baseline model saved to ncf_baseline_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:15:42,629 - INFO - Starting ALS models evaluation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 19:15:42,651 - INFO - Evaluating budget-aware ALS model...\n",
      "2025-07-25 19:20:07,117 - INFO - ALS RMSE: 0.8027380664203994, Precision@10: 0.4434385788632965, Recall@10: 0.8338646700175392, NDCG@10: 0.8836108559971472\n",
      "2025-07-25 19:20:07,369 - INFO - Evaluating ALS CV model...\n",
      "2025-07-25 19:25:01,769 - INFO - ALS CV RMSE: 0.7986708117672686, Precision@10: 0.4438688359870148, Recall@10: 0.8341365775660732, NDCG@10: 0.8842670488375413\n",
      "2025-07-25 19:25:01,860 - INFO - Evaluating ALS baseline model...\n",
      "2025-07-25 19:30:23,919 - INFO - ALS Baseline RMSE: 0.818098461609045, Precision@10: 0.4994386689880877, Recall@10: 0.7854035140796781, NDCG@10: 0.8788548384043531\n",
      "2025-07-25 19:30:24,026 - INFO - ALS evaluation results saved to als_evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, collect_list, array_sort, desc, expr\n",
    "\n",
    "def evaluate_als_models(als_model, als_model_cv, als_baseline_model, test_df, k=10, results_file=\"als_evaluation_results.txt\"):\n",
    "    \"\"\"\n",
    "    Evaluate ALS models (budget-aware, CV, and baseline) using RMSE, precision@k, recall@k, and NDCG@k.\n",
    "    \n",
    "    Args:\n",
    "        als_model_path: Path to saved budget-aware ALS model\n",
    "        als_model_cv_path: Path to saved CV ALS model\n",
    "        als_baseline_model: Trained baseline ALS model\n",
    "        test_df: Spark DataFrame with test data\n",
    "        k: Value for ranking metrics (default: 10)\n",
    "        results_file: Path to save evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting ALS models evaluation...\")\n",
    "\n",
    "        # Initialize evaluators\n",
    "        rmse_evaluator = RegressionEvaluator(\n",
    "            metricName=\"rmse\", labelCol=\"rating\", predictionCol=\"prediction\"\n",
    "        )\n",
    "        ranking_evaluator = RankingEvaluator(\n",
    "            predictionCol=\"prediction\", labelCol=\"rating\", k=k\n",
    "        )\n",
    "\n",
    "        # Compute ranking metrics function\n",
    "        def compute_ranking_metrics(predictions, k=10):\n",
    "            window = Window.partitionBy(\"userId\").orderBy(col(\"prediction\").desc())\n",
    "            pred_top_k = predictions.withColumn(\"rank\", row_number().over(window)) \\\n",
    "                .where(col(\"rank\") <= k) \\\n",
    "                .groupBy(\"userId\") \\\n",
    "                .agg(collect_list(\"movieId\").alias(\"predicted_movies\"))\n",
    "\n",
    "            actual_top_k = predictions.where(col(\"rating\") >= 4.0) \\\n",
    "                .groupBy(\"userId\") \\\n",
    "                .agg(collect_list(\"movieId\").alias(\"actual_movies\"))\n",
    "\n",
    "            ranking_df = pred_top_k.join(actual_top_k, on=\"userId\", how=\"inner\") \\\n",
    "                .select(\n",
    "                    expr(\"transform(predicted_movies, x -> cast(x as double))\").alias(\"prediction\"),\n",
    "                    expr(\"transform(actual_movies, x -> cast(x as double))\").alias(\"rating\")\n",
    "                )\n",
    "\n",
    "            ranking_evaluator = RankingEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", k=k)\n",
    "            precision_k = ranking_evaluator.setMetricName(\"precisionAtK\").evaluate(ranking_df)\n",
    "            recall_k = ranking_evaluator.setMetricName(\"recallAtK\").evaluate(ranking_df)\n",
    "            ndcg_k = ranking_evaluator.setMetricName(\"ndcgAtK\").evaluate(ranking_df)\n",
    "            return precision_k, recall_k, ndcg_k\n",
    "\n",
    "        # Evaluate ALS (budget-aware)\n",
    "        logging.info(\"Evaluating budget-aware ALS model...\")\n",
    "        als_predictions = als_model.transform(test_df)\n",
    "        als_rmse = rmse_evaluator.evaluate(als_predictions)\n",
    "        als_precision, als_recall, als_ndcg = compute_ranking_metrics(\n",
    "            als_predictions)\n",
    "        logging.info(\n",
    "            f\"ALS RMSE: {als_rmse}, Precision@{k}: {als_precision}, Recall@{k}: {als_recall}, NDCG@{k}: {als_ndcg}\")\n",
    "\n",
    "        # Evaluate ALS CV\n",
    "        logging.info(\"Evaluating ALS CV model...\")\n",
    "        als_cv_predictions = als_model_cv.transform(test_df)\n",
    "        als_cv_rmse = rmse_evaluator.evaluate(als_cv_predictions)\n",
    "        als_cv_precision, als_cv_recall, als_cv_ndcg = compute_ranking_metrics(\n",
    "            als_cv_predictions)\n",
    "        logging.info(\n",
    "            f\"ALS CV RMSE: {als_cv_rmse}, Precision@{k}: {als_cv_precision}, Recall@{k}: {als_cv_recall}, NDCG@{k}: {als_cv_ndcg}\")\n",
    "\n",
    "        # Evaluate ALS baseline\n",
    "        logging.info(\"Evaluating ALS baseline model...\")\n",
    "        als_baseline_predictions = als_baseline_model.transform(test_df)\n",
    "        als_baseline_rmse = rmse_evaluator.evaluate(als_baseline_predictions)\n",
    "        als_baseline_precision, als_baseline_recall, als_baseline_ndcg = compute_ranking_metrics(\n",
    "            als_baseline_predictions)\n",
    "        logging.info(\n",
    "            f\"ALS Baseline RMSE: {als_baseline_rmse}, Precision@{k}: {als_baseline_precision}, Recall@{k}: {als_baseline_recall}, NDCG@{k}: {als_baseline_ndcg}\")\n",
    "\n",
    "        # Save results\n",
    "        with open(results_file, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"ALS RMSE: {als_rmse}\\nPrecision@{k}: {als_precision}\\nRecall@{k}: {als_recall}\\nNDCG@{k}: {als_ndcg}\\n\")\n",
    "            f.write(\n",
    "                f\"ALS CV RMSE: {als_cv_rmse}\\nPrecision@{k}: {als_cv_precision}\\nRecall@{k}: {als_cv_recall}\\nNDCG@{k}: {als_cv_ndcg}\\n\")\n",
    "            f.write(\n",
    "                f\"ALS Baseline RMSE: {als_baseline_rmse}\\nPrecision@{k}: {als_baseline_precision}\\nRecall@{k}: {als_baseline_recall}\\nNDCG@{k}: {als_baseline_ndcg}\\n\")\n",
    "        logging.info(f\"ALS evaluation results saved to {results_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to evaluate ALS models: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Call the function\n",
    "evaluate_als_models(\n",
    "    als_model,\n",
    "    als_model_cv,  \n",
    "    als_baseline_model,\n",
    "    test_df=test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open(\"user_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_map, f)\n",
    "with open(\"movie_map.pkl\", \"wb\") as f:\n",
    "    pickle.dump(movie_map, f)\n",
    "with open(\"user_map_baseline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(user_map_baseline, f)\n",
    "with open(\"movie_map_baseline.pkl\", \"wb\") as f:\n",
    "    pickle.dump(movie_map_baseline, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save\n",
    "train_df.write.mode(\"overwrite\").parquet(\"train_df.parquet\")\n",
    "test_df.write.mode(\"overwrite\").parquet(\"test_df.parquet\")\n",
    "movies_df.write.mode(\"overwrite\").parquet(\"movies_df.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user_map length: 221652\n",
      "movie_map length: 19108\n",
      "user_map_baseline length: 328892\n",
      "movie_map_baseline length: 32021\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "with open(\"user_map.pkl\", \"rb\") as f:\n",
    "    user_map = pickle.load(f)\n",
    "with open(\"movie_map.pkl\", \"rb\") as f:\n",
    "    movie_map = pickle.load(f)\n",
    "with open(\"user_map_baseline.pkl\", \"rb\") as f:\n",
    "    user_map_baseline = pickle.load(f)\n",
    "with open(\"movie_map_baseline.pkl\", \"rb\") as f:\n",
    "    movie_map_baseline = pickle.load(f)\n",
    "\n",
    "print(f\"user_map length: {len(user_map)}\")\n",
    "print(f\"movie_map length: {len(movie_map)}\")\n",
    "print(f\"user_map_baseline length: {len(user_map_baseline)}\")\n",
    "print(f\"movie_map_baseline length: {len(movie_map_baseline)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 19:58:51,018 - INFO - Starting NCF models evaluation...\n",
      "2025-08-02 20:00:20,622 - INFO - Test data size: 2828066 ratings\n",
      "2025-08-02 20:00:21,650 - INFO - Evaluating budget-aware NCF model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11024/11024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 2ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 20:00:48,235 - INFO - Evaluating NCF baseline model...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11024/11024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-02 20:01:28,894 - INFO - Computing ranking metrics for NCF...\n",
      "2025-08-02 20:06:38,186 - INFO - NCF RMSE: 0.8467916130875904, Precision@10: 0.4350195805637138, Recall@10: 0.8241554161915556, NDCG@10: 0.8673054665417687\n",
      "2025-08-02 20:06:38,186 - INFO - Computing ranking metrics for NCF baseline...\n",
      "2025-08-02 20:09:15,163 - INFO - NCF Baseline RMSE: 0.8458766979382569, Precision@10: 0.4363805585613429, Recall@10: 0.8252542085592518, NDCG@10: 0.8713218084278783\n",
      "2025-08-02 20:09:15,165 - INFO - NCF evaluation results saved to ncf_evaluation_results.txt\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.ml.evaluation import RankingEvaluator\n",
    "import os\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import col, row_number, collect_list, array_sort, desc, expr\n",
    "\n",
    "import pickle\n",
    "ncf_model = load_model(\"ncf_model.h5\", compile =False)\n",
    "ncf_baseline_model = load_model(\"ncf_baseline_model.h5\", compile=False)\n",
    "\n",
    "with open(\"user_map.pkl\", \"rb\") as f:\n",
    "    user_map = pickle.load(f)\n",
    "with open(\"movie_map.pkl\", \"rb\") as f:\n",
    "    movie_map = pickle.load(f)\n",
    "with open(\"user_map_baseline.pkl\", \"rb\") as f:\n",
    "    user_map_baseline = pickle.load(f)\n",
    "with open(\"movie_map_baseline.pkl\", \"rb\") as f:\n",
    "    movie_map_baseline = pickle.load(f)\n",
    "test_df = test_df.repartition(50)\n",
    "\n",
    "def evaluate_ncf_models(ncf_model, ncf_baseline_model, user_map, movie_map, user_map_baseline, movie_map_baseline, test_df, movies_df, k=10, results_file=\"ncf_evaluation_results.txt\"):\n",
    "    \"\"\"\n",
    "    Evaluate NCF models (budget-aware and baseline) using RMSE, precision@k, recall@k, and NDCG@k.\n",
    "    \n",
    "    Args:\n",
    "        ncf_model_path: Path to saved budget-aware NCF model0\n",
    "        ncf_baseline_model: Trained baseline NCF model\n",
    "        user_map: Mapping for budget-aware model user IDs\n",
    "        movie_map: Mapping for budget-aware model movie IDs\n",
    "        user_map_baseline: Mapping for baseline model user IDs\n",
    "        movie_map_baseline: Mapping for baseline model movie IDs\n",
    "        test_df: Spark DataFrame with test data\n",
    "        movies_df: Spark DataFrame with movie data\n",
    "        k: Value for ranking metrics (default: 10)\n",
    "        results_file: Path to save evaluation results\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logging.info(\"Starting NCF models evaluation...\")\n",
    "        # Load models\n",
    "        \n",
    "        # Convert test data to Pandas\n",
    "        test_pd = test_df.join(movies_df.select(\n",
    "            \"movieId\", \"budget_normalized\"), \"movieId\").toPandas()\n",
    "        logging.info(f\"Test data size: {len(test_pd)} ratings\")\n",
    "        test_pd[\"user_idx\"] = test_pd[\"userId\"].map(user_map)\n",
    "        test_pd[\"movie_idx\"] = test_pd[\"movieId\"].map(movie_map)\n",
    "        test_pd[\"user_idx_baseline\"] = test_pd[\"userId\"].map(user_map_baseline)\n",
    "        test_pd[\"movie_idx_baseline\"] = test_pd[\"movieId\"].map(\n",
    "            movie_map_baseline)\n",
    "\n",
    "        \n",
    "        # Filter out rows with missing indices\n",
    "        test_pd = test_pd.dropna(subset=[\"user_idx\", \"movie_idx\", \"user_idx_baseline\", \"movie_idx_baseline\"])\n",
    "\n",
    "        # Convert indices to int for TensorFlow compatibility\n",
    "        test_pd[\"user_idx\"] = test_pd[\"user_idx\"].astype(int)\n",
    "        test_pd[\"movie_idx\"] = test_pd[\"movie_idx\"].astype(int)\n",
    "        test_pd[\"user_idx_baseline\"] = test_pd[\"user_idx_baseline\"].astype(int)\n",
    "        test_pd[\"movie_idx_baseline\"] = test_pd[\"movie_idx_baseline\"].astype(int)\n",
    "\n",
    "\n",
    "        # Evaluate NCF (budget-aware)\n",
    "        logging.info(\"Evaluating budget-aware NCF model...\")\n",
    "        ncf_predictions = ncf_model.predict(\n",
    "            [test_pd[\"user_idx\"], test_pd[\"movie_idx\"],\n",
    "                test_pd[\"budget_normalized\"] / test_pd[\"budget_normalized\"].max()],\n",
    "            batch_size=256,\n",
    "            verbose=1\n",
    "        )\n",
    "        ncf_rmse = np.sqrt(mean_squared_error(\n",
    "            test_pd[\"rating\"], ncf_predictions.flatten() * 5.0))\n",
    "\n",
    "        # Evaluate NCF baseline\n",
    "        logging.info(\"Evaluating NCF baseline model...\")\n",
    "        ncf_baseline_predictions = ncf_baseline_model.predict(\n",
    "            [test_pd[\"user_idx_baseline\"], test_pd[\"movie_idx_baseline\"]],\n",
    "            batch_size=256,\n",
    "            verbose=1\n",
    "        )\n",
    "        ncf_baseline_rmse = np.sqrt(mean_squared_error(\n",
    "            test_pd[\"rating\"], ncf_baseline_predictions.flatten() * 5.0))\n",
    "\n",
    "        # Compute ranking metrics\n",
    "        def compute_ncf_ranking_metrics(predictions, test_pd, spark, k=k):\n",
    "            # Create DataFrame with predictions\n",
    "            pred_df = pd.DataFrame({\n",
    "                \"userId\": test_pd[\"userId\"],\n",
    "                \"movieId\": test_pd[\"movieId\"],\n",
    "                \"rating\": test_pd[\"rating\"],\n",
    "                \"prediction\": predictions.flatten() * 5.0\n",
    "            })\n",
    "            spark_pred_df = spark.createDataFrame(pred_df)\n",
    "\n",
    "            # Get top-k predicted movies per user\n",
    "            window = Window.partitionBy(\"userId\").orderBy(col(\"prediction\").desc())\n",
    "            pred_top_k = spark_pred_df.withColumn(\"rank\", row_number().over(window)) \\\n",
    "                .where(col(\"rank\") <= k) \\\n",
    "                .groupBy(\"userId\") \\\n",
    "                .agg(collect_list(\"movieId\").alias(\"predicted_movies\"))\n",
    "\n",
    "            # Get actual relevant movies per user (e.g., rating >= 4.0)\n",
    "            actual_top_k = spark_pred_df.where(col(\"rating\") >= 4.0) \\\n",
    "                .groupBy(\"userId\") \\\n",
    "                .agg(collect_list(\"movieId\").alias(\"actual_movies\"))\n",
    "\n",
    "            # Join and cast to array<double>\n",
    "            ranking_df = pred_top_k.join(actual_top_k, on=\"userId\", how=\"inner\") \\\n",
    "                .select(\n",
    "                    expr(\"transform(predicted_movies, x -> cast(x as double))\").alias(\"prediction\"),\n",
    "                    expr(\"transform(actual_movies, x -> cast(x as double))\").alias(\"rating\")\n",
    "                )\n",
    "\n",
    "            ranking_evaluator = RankingEvaluator(predictionCol=\"prediction\", labelCol=\"rating\", k=k)\n",
    "            precision_k = ranking_evaluator.setMetricName(\"precisionAtK\").evaluate(ranking_df)\n",
    "            recall_k = ranking_evaluator.setMetricName(\"recallAtK\").evaluate(ranking_df)\n",
    "            ndcg_k = ranking_evaluator.setMetricName(\"ndcgAtK\").evaluate(ranking_df)\n",
    "            return precision_k, recall_k, ndcg_k\n",
    "\n",
    "        logging.info(\"Computing ranking metrics for NCF...\")\n",
    "        ncf_precision, ncf_recall, ncf_ndcg = compute_ncf_ranking_metrics(\n",
    "            ncf_predictions, test_pd, test_df.sparkSession)\n",
    "        logging.info(\n",
    "            f\"NCF RMSE: {ncf_rmse}, Precision@{k}: {ncf_precision}, Recall@{k}: {ncf_recall}, NDCG@{k}: {ncf_ndcg}\")\n",
    "\n",
    "        logging.info(\"Computing ranking metrics for NCF baseline...\")\n",
    "        ncf_baseline_precision, ncf_baseline_recall, ncf_baseline_ndcg = compute_ncf_ranking_metrics(\n",
    "            ncf_baseline_predictions, test_pd, test_df.sparkSession)\n",
    "        logging.info(\n",
    "            f\"NCF Baseline RMSE: {ncf_baseline_rmse}, Precision@{k}: {ncf_baseline_precision}, Recall@{k}: {ncf_baseline_recall}, NDCG@{k}: {ncf_baseline_ndcg}\")\n",
    "\n",
    "        # Save results\n",
    "        with open(results_file, \"w\") as f:\n",
    "            f.write(\n",
    "                f\"NCF RMSE: {ncf_rmse}\\nPrecision@{k}: {ncf_precision}\\nRecall@{k}: {ncf_recall}\\nNDCG@{k}: {ncf_ndcg}\\n\")\n",
    "            f.write(\n",
    "                f\"NCF Baseline RMSE: {ncf_baseline_rmse}\\nPrecision@{k}: {ncf_baseline_precision}\\nRecall@{k}: {ncf_baseline_recall}\\nNDCG@{k}: {ncf_baseline_ndcg}\\n\")\n",
    "        logging.info(f\"NCF evaluation results saved to {results_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to evaluate NCF models: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Call the function\n",
    "evaluate_ncf_models(\n",
    "    ncf_model,\n",
    "    ncf_baseline_model,\n",
    "    user_map,\n",
    "    movie_map,\n",
    "    user_map_baseline,\n",
    "    movie_map_baseline,\n",
    "    test_df,\n",
    "    movies_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------+------+----------+-----+\n",
      "|movieId|userId|rating| timestamp|count|\n",
      "+-------+------+------+----------+-----+\n",
      "|     28|   224|   3.0|1123310812| 3497|\n",
      "|     28|   388|   4.0| 950162252| 3497|\n",
      "|     28|   663|   4.0|1120519874| 3497|\n",
      "|     28|  1617|   4.5|1505372662| 3497|\n",
      "|     28|  1821|   4.0| 943907496| 3497|\n",
      "+-------+------+------+----------+-----+\n",
      "only showing top 5 rows\n",
      "+--------------------+-------+--------------------+---------+------------+------------------+\n",
      "|              genres|movieId|               title|   budget|release_date| budget_normalized|\n",
      "+--------------------+-------+--------------------+---------+------------+------------------+\n",
      "|       Drama,Fantasy| 167420|  Master i Margarita|      0.0|  1994-06-06|               0.0|\n",
      "|Adventure,Animati...|   3751|         Chicken Run|    4.5E7|  2000-06-23|17.622173047734595|\n",
      "|Biography,Drama,R...|   5791|               Frida|    1.2E7|  2002-08-29|16.300417207752275|\n",
      "|Action,Adventure,...|  33493|Star Wars: Episod...|   1.13E8|  2005-05-17|18.542898376676614|\n",
      "|Drama,Mystery,Rom...|   3457|     Waking the Dead|8500000.0|  2000-03-24|15.955576721460545|\n",
      "+--------------------+-------+--------------------+---------+------------+------------------+\n",
      "only showing top 5 rows\n"
     ]
    }
   ],
   "source": [
    "test_df.show(5)\n",
    "movies_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 01:09:29,131 - INFO - Starting statistical and fairness analysis...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m11024/11024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 3ms/step\n",
      "\u001b[1m11024/11024\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 3ms/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 01:12:48,114 - INFO - Performing paired t-tests with sample size 1000...\n",
      "2025-08-04 01:18:54,974 - INFO - ALS vs ALS CV t-test: t=27.1396, p=0.0000\n",
      "2025-08-04 01:18:55,013 - INFO - ALS vs ALS Baseline t-test: t=-0.1304, p=0.8963\n",
      "2025-08-04 01:18:55,023 - INFO - ALS vs NCF t-test: t=4.9065, p=0.0000\n",
      "2025-08-04 01:18:55,023 - INFO - ALS CV vs ALS Baseline t-test: t=-0.6582, p=0.5106\n",
      "2025-08-04 01:18:55,037 - INFO - ALS CV vs NCF t-test: t=4.4087, p=0.0000\n",
      "2025-08-04 01:18:55,037 - INFO - ALS Baseline vs NCF t-test: t=5.0701, p=0.0000\n",
      "2025-08-04 01:18:55,037 - INFO - NCF vs NCF Baseline t-test: t=1.6081, p=0.1081\n",
      "2025-08-04 01:18:55,037 - INFO - Performing fairness analysis (diversity across budget tiers)...\n",
      "2025-08-04 01:20:22,911 - INFO - Budget quartiles (budget > 0): [1300000.0, 5150000.0, 17000000.0]\n",
      "2025-08-04 01:21:41,218 - INFO - ALS Diversity by budget tier: [Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='High', count=1), Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='Medium', count=2)]\n",
      "2025-08-04 01:22:52,729 - INFO - ALS CV Diversity by budget tier: [Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='High', count=2), Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='Medium', count=1)]\n",
      "2025-08-04 01:24:19,858 - INFO - ALS Baseline Diversity by budget tier: [Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='High', count=1)]\n",
      "2025-08-04 01:26:28,646 - INFO - NCF Diversity by budget tier: [Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='High', count=9)]\n",
      "2025-08-04 01:30:30,566 - INFO - NCF Baseline Diversity by budget tier: [Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='High', count=3), Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='Low', count=2), Row(CASE WHEN (budget <= 1300000.0) THEN Low WHEN (budget <= 5150000.0) THEN Medium ELSE High END='Medium', count=1)]\n",
      "2025-08-04 01:30:30,578 - INFO - Statistical and fairness analysis results saved to statistical_fairness_results.txt\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import load_model\n",
    "from scipy.stats import ttest_rel\n",
    "from pyspark.sql.functions import col, when\n",
    "import os\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "import pickle\n",
    "ncf_model = load_model(\"ncf_model.h5\", compile=False)\n",
    "ncf_baseline_model = load_model(\"ncf_baseline_model.h5\", compile=False)\n",
    "# Load the ALS models\n",
    "als_model = ALSModel.load(\"als_model\")\n",
    "als_baseline_model = ALSModel.load(\"als_baseline_model\")\n",
    "als_model_cv = ALSModel.load(\"als_model_cv\")\n",
    "\n",
    "\n",
    "with open(\"user_map.pkl\", \"rb\") as f:\n",
    "    user_map = pickle.load(f)\n",
    "with open(\"movie_map.pkl\", \"rb\") as f:\n",
    "    movie_map = pickle.load(f)\n",
    "with open(\"user_map_baseline.pkl\", \"rb\") as f:\n",
    "    user_map_baseline = pickle.load(f)\n",
    "with open(\"movie_map_baseline.pkl\", \"rb\") as f:\n",
    "    movie_map_baseline = pickle.load(f)\n",
    "test_df = test_df.repartition(50)\n",
    "\n",
    "def statistical_and_fairness_analysis(\n",
    "    als_model, als_model_cv, als_baseline_model, ncf_model, ncf_baseline_model,\n",
    "    user_map, movie_map, user_map_baseline, movie_map_baseline,\n",
    "    test_df, movies_df, sample_size=1000, results_file=\"statistical_fairness_results.txt\"\n",
    "):\n",
    "    try:\n",
    "        logging.info(\"Starting statistical and fairness analysis...\")\n",
    "\n",
    "        als_predictions = als_model.transform(test_df)\n",
    "        als_cv_predictions = als_model_cv.transform(test_df)\n",
    "        als_baseline_predictions = als_baseline_model.transform(test_df)\n",
    "\n",
    "        test_pd = test_df.join(movies_df.select(\n",
    "            \"movieId\", \"budget_normalized\"), \"movieId\").toPandas()\n",
    "        test_pd[\"user_idx\"] = test_pd[\"userId\"].map(user_map)\n",
    "        test_pd[\"movie_idx\"] = test_pd[\"movieId\"].map(movie_map)\n",
    "        test_pd[\"user_idx_baseline\"] = test_pd[\"userId\"].map(user_map_baseline)\n",
    "        test_pd[\"movie_idx_baseline\"] = test_pd[\"movieId\"].map(\n",
    "            movie_map_baseline)\n",
    "        \n",
    "\n",
    "        # Drop any rows where mapping failed\n",
    "        test_pd = test_pd.dropna(subset=[\n",
    "        \"user_idx\", \"movie_idx\", \"user_idx_baseline\", \"movie_idx_baseline\"\n",
    "        ])\n",
    "\n",
    "        # Ensure integer indices for embedding layers\n",
    "        test_pd[\"user_idx\"] = test_pd[\"user_idx\"].astype(int)\n",
    "        test_pd[\"movie_idx\"] = test_pd[\"movie_idx\"].astype(int)\n",
    "        test_pd[\"user_idx_baseline\"] = test_pd[\"user_idx_baseline\"].astype(int)\n",
    "        test_pd[\"movie_idx_baseline\"] = test_pd[\"movie_idx_baseline\"].astype(int)\n",
    "\n",
    "        ncf_predictions = ncf_model.predict(\n",
    "            [test_pd[\"user_idx\"], test_pd[\"movie_idx\"],\n",
    "             test_pd[\"budget_normalized\"] / test_pd[\"budget_normalized\"].max()],\n",
    "            batch_size=256,\n",
    "            verbose=1\n",
    "        )\n",
    "        ncf_baseline_predictions = ncf_baseline_model.predict(\n",
    "            [test_pd[\"user_idx_baseline\"], test_pd[\"movie_idx_baseline\"]],\n",
    "            batch_size=256,\n",
    "            verbose=1\n",
    "        )\n",
    "\n",
    "        logging.info(\n",
    "            f\"Performing paired t-tests with sample size {sample_size}...\")\n",
    "        als_errors = als_predictions.select(\n",
    "            (col(\"rating\") - col(\"prediction\")).alias(\"error\")).rdd.map(lambda x: x.error).collect()\n",
    "        als_cv_errors = als_cv_predictions.select(\n",
    "            (col(\"rating\") - col(\"prediction\")).alias(\"error\")).rdd.map(lambda x: x.error).collect()\n",
    "        als_baseline_errors = als_baseline_predictions.select(\n",
    "            (col(\"rating\") - col(\"prediction\")).alias(\"error\")).rdd.map(lambda x: x.error).collect()\n",
    "        ncf_errors = (test_pd[\"rating\"] -\n",
    "                      ncf_predictions.flatten() * 5.0).tolist()\n",
    "        ncf_baseline_errors = (\n",
    "            test_pd[\"rating\"] - ncf_baseline_predictions.flatten() * 5.0).tolist()\n",
    "\n",
    "        sample_size = min(sample_size, len(als_errors), len(als_cv_errors), len(\n",
    "            als_baseline_errors), len(ncf_errors), len(ncf_baseline_errors))\n",
    "        als_errors = als_errors[:sample_size]\n",
    "        als_cv_errors = als_cv_errors[:sample_size]\n",
    "        als_baseline_errors = als_baseline_errors[:sample_size]\n",
    "        ncf_errors = ncf_errors[:sample_size]\n",
    "        ncf_baseline_errors = ncf_baseline_errors[:sample_size]\n",
    "\n",
    "        comparisons = [\n",
    "            (\"ALS vs ALS CV\", als_errors, als_cv_errors),\n",
    "            (\"ALS vs ALS Baseline\", als_errors, als_baseline_errors),\n",
    "            (\"ALS vs NCF\", als_errors, ncf_errors),\n",
    "            (\"ALS CV vs ALS Baseline\", als_cv_errors, als_baseline_errors),\n",
    "            (\"ALS CV vs NCF\", als_cv_errors, ncf_errors),\n",
    "            (\"ALS Baseline vs NCF\", als_baseline_errors, ncf_errors),\n",
    "            (\"NCF vs NCF Baseline\", ncf_errors, ncf_baseline_errors)\n",
    "        ]\n",
    "        t_test_results = []\n",
    "        for name, errors1, errors2 in comparisons:\n",
    "            t_stat, p_value = ttest_rel(errors1, errors2)\n",
    "            t_test_results.append((name, t_stat, p_value))\n",
    "            logging.info(f\"{name} t-test: t={t_stat:.4f}, p={p_value:.4f}\")\n",
    "\n",
    "        logging.info(\n",
    "            \"Performing fairness analysis (diversity across budget tiers)...\")\n",
    "        # Filter movies_df to exclude zero or negative budgets\n",
    "        movies_with_budget_df = movies_df.filter(col(\"budget\") > 0)\n",
    "        # Compute budget quartiles from valid budgets only\n",
    "        budget_quartiles = movies_with_budget_df.approxQuantile(\n",
    "            \"budget\", [0.25, 0.5, 0.75], 0.05\n",
    "        )\n",
    "        logging.info(f\"Budget quartiles (budget > 0): {budget_quartiles}\")\n",
    "\n",
    "        # Redefine compute_diversity to use filtered movies_with_budget_df\n",
    "        def compute_diversity(predictions, model_name, k=10):\n",
    "            diversity = predictions.orderBy(col(\"prediction\").desc()).limit(k).join(\n",
    "                movies_with_budget_df, \"movieId\"\n",
    "            ).groupBy(\n",
    "                when(col(\"budget\") <= budget_quartiles[0], \"Low\")\n",
    "                .when(col(\"budget\") <= budget_quartiles[1], \"Medium\")\n",
    "                .otherwise(\"High\")\n",
    "            ).count()\n",
    "            diversity_result = diversity.collect()\n",
    "            logging.info(f\"{model_name} Diversity by budget tier: {diversity_result}\")\n",
    "            return diversity_result\n",
    "\n",
    "        als_diversity = compute_diversity(als_predictions, \"ALS\")\n",
    "        als_cv_diversity = compute_diversity(als_cv_predictions, \"ALS CV\")\n",
    "        als_baseline_diversity = compute_diversity(\n",
    "            als_baseline_predictions, \"ALS Baseline\")\n",
    "\n",
    "        ncf_pred_df = pd.DataFrame({\n",
    "            \"userId\": test_pd[\"userId\"],\n",
    "            \"movieId\": test_pd[\"movieId\"],\n",
    "            \"rating\": test_pd[\"rating\"],\n",
    "            \"prediction\": ncf_predictions.flatten() * 5.0\n",
    "        })\n",
    "        ncf_spark_pred_df = test_df.sparkSession.createDataFrame(ncf_pred_df)\n",
    "        ncf_diversity = compute_diversity(ncf_spark_pred_df, \"NCF\")\n",
    "\n",
    "        ncf_baseline_pred_df = pd.DataFrame({\n",
    "            \"userId\": test_pd[\"userId\"],\n",
    "            \"movieId\": test_pd[\"movieId\"],\n",
    "            \"rating\": test_pd[\"rating\"],\n",
    "            \"prediction\": ncf_baseline_predictions.flatten() * 5.0\n",
    "        })\n",
    "        ncf_baseline_spark_pred_df = test_df.sparkSession.createDataFrame(\n",
    "            ncf_baseline_pred_df)\n",
    "        ncf_baseline_diversity = compute_diversity(\n",
    "            ncf_baseline_spark_pred_df, \"NCF Baseline\")\n",
    "\n",
    "        with open(results_file, \"w\") as f:\n",
    "            f.write(\"Statistical Tests (Paired t-tests):\\n\")\n",
    "            for name, t_stat, p_value in t_test_results:\n",
    "                f.write(f\"{name}: t={t_stat:.4f}, p={p_value:.4f}\\n\")\n",
    "            f.write(\"\\nFairness Analysis (Diversity by Budget Tier):\\n\")\n",
    "            f.write(f\"ALS: {als_diversity}\\n\")\n",
    "            f.write(f\"ALS CV: {als_cv_diversity}\\n\")\n",
    "            f.write(f\"ALS Baseline: {als_baseline_diversity}\\n\")\n",
    "            f.write(f\"NCF: {ncf_diversity}\\n\")\n",
    "            f.write(f\"NCF Baseline: {ncf_baseline_diversity}\\n\")\n",
    "\n",
    "        logging.info(\n",
    "            f\"Statistical and fairness analysis results saved to {results_file}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        logging.error(\n",
    "            f\"Failed to perform statistical and fairness analysis: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "# Call the function\n",
    "statistical_and_fairness_analysis(\n",
    "    als_model,\n",
    "    als_model_cv,\n",
    "    als_baseline_model,\n",
    "    ncf_model,\n",
    "    ncf_baseline_model,\n",
    "    user_map=user_map,\n",
    "    movie_map=movie_map,\n",
    "    user_map_baseline=user_map_baseline,\n",
    "    movie_map_baseline=movie_map_baseline,\n",
    "    test_df=test_df,\n",
    "    movies_df=movies_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 12:37:17,973 - INFO - Visualizations saved: rmse_comparison.png, ranking_metrics.png, statistical_tests.png, diversity_comparison.png\n",
      "2025-08-04 12:37:17,973 - INFO - Visualizations saved: rmse_comparison.png, ranking_metrics.png, statistical_tests.png, diversity_comparison.png\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Visualize Results\n",
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def parse_evaluation_results(file_path, model_name):\n",
    "    metrics = {}\n",
    "    with open(file_path, 'r') as f:\n",
    "        content = f.read()\n",
    "        for metric in ['RMSE', 'Precision@10', 'Recall@10', 'NDCG@10']:\n",
    "            match = re.search(rf\"{model_name}\\s*{metric}:\\s*([\\d.]+)\", content)\n",
    "            if match:\n",
    "                metrics[metric] = float(match.group(1))\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def parse_statistical_fairness(file_path):\n",
    "    stats = {}\n",
    "    fairness = {}\n",
    "    current_section = None\n",
    "    with open(file_path, 'r') as f:\n",
    "        for line in f:\n",
    "            line = line.strip()\n",
    "            if line.startswith(\"Statistical Tests\"):\n",
    "                current_section = \"stats\"\n",
    "                continue\n",
    "            elif line.startswith(\"Fairness Analysis\"):\n",
    "                current_section = \"fairness\"\n",
    "                continue\n",
    "            if current_section == \"stats\" and line.startswith(\"ALS\") or line.startswith(\"NCF\"):\n",
    "                match = re.search(\n",
    "                    r\"(\\w+\\s*(?:CV|Baseline)?)\\s*vs\\s*(\\w+\\s*(?:CV|Baseline)?):\\s*t=([\\d.-]+),\\s*p=([\\d.e-]+)\", line)\n",
    "                if match:\n",
    "                    key = f\"{match.group(1)} vs {match.group(2)}\"\n",
    "                    stats[key] = {'t': float(match.group(\n",
    "                        3)), 'p': float(match.group(4))}\n",
    "            elif current_section == \"fairness\" and line.startswith(\"ALS\") or line.startswith(\"NCF\"):\n",
    "                match = re.search(\n",
    "                    r\"(\\w+\\s*(?:CV|Baseline)?):\\s*\\[(.*?)\\]\", line)\n",
    "                if match:\n",
    "                    model = match.group(1)\n",
    "                    rows = re.findall(\n",
    "                        r\"Row\\(.*?='(\\w+)',\\s*count=(\\d+)\\)\", match.group(2))\n",
    "                    fairness[model] = {tier: int(count)\n",
    "                                       for tier, count in rows}\n",
    "    return stats, fairness\n",
    "\n",
    "\n",
    "def visualize_results():\n",
    "    try:\n",
    "        # Parse evaluation results\n",
    "        als_metrics = parse_evaluation_results(\n",
    "            \"als_evaluation_results.txt\", \"ALS\")\n",
    "        als_cv_metrics = parse_evaluation_results(\n",
    "            \"als_evaluation_results.txt\", \"ALS CV\")\n",
    "        als_baseline_metrics = parse_evaluation_results(\n",
    "            \"als_evaluation_results.txt\", \"ALS Baseline\")\n",
    "        ncf_metrics = parse_evaluation_results(\n",
    "            \"ncf_evaluation_results.txt\", \"NCF\")\n",
    "        ncf_baseline_metrics = parse_evaluation_results(\n",
    "            \"ncf_evaluation_results.txt\", \"NCF Baseline\")\n",
    "\n",
    "        # Parse statistical and fairness results\n",
    "        stats_results, fairness_results = parse_statistical_fairness(\n",
    "            \"statistical_fairness_results.txt\")\n",
    "\n",
    "        # Combine metrics into a DataFrame\n",
    "        metrics_df = pd.DataFrame({\n",
    "            \"Model\": [\"ALS\", \"ALS CV\", \"ALS Baseline\", \"NCF\", \"NCF Baseline\"],\n",
    "            \"RMSE\": [als_metrics.get(\"RMSE\", 0), als_cv_metrics.get(\"RMSE\", 0), als_baseline_metrics.get(\"RMSE\", 0),\n",
    "                     ncf_metrics.get(\"RMSE\", 0), ncf_baseline_metrics.get(\"RMSE\", 0)],\n",
    "            \"Precision@10\": [als_metrics.get(\"Precision@10\", 0), als_cv_metrics.get(\"Precision@10\", 0),\n",
    "                             als_baseline_metrics.get(\n",
    "                                 \"Precision@10\", 0), ncf_metrics.get(\"Precision@10\", 0),\n",
    "                             ncf_baseline_metrics.get(\"Precision@10\", 0)],\n",
    "            \"Recall@10\": [als_metrics.get(\"Recall@10\", 0), als_cv_metrics.get(\"Recall@10\", 0),\n",
    "                          als_baseline_metrics.get(\n",
    "                              \"Recall@10\", 0), ncf_metrics.get(\"Recall@10\", 0),\n",
    "                          ncf_baseline_metrics.get(\"Recall@10\", 0)],\n",
    "            \"NDCG@10\": [als_metrics.get(\"NDCG@10\", 0), als_cv_metrics.get(\"NDCG@10\", 0),\n",
    "                        als_baseline_metrics.get(\n",
    "                            \"NDCG@10\", 0), ncf_metrics.get(\"NDCG@10\", 0),\n",
    "                        ncf_baseline_metrics.get(\"NDCG@10\", 0)]\n",
    "        })\n",
    "\n",
    "        # Plot RMSE comparison\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=\"Model\", y=\"RMSE\", data=metrics_df)\n",
    "        plt.title(\"RMSE Comparison: Budget-Aware vs. Baseline Models\")\n",
    "        plt.ylabel(\"RMSE\")\n",
    "        plt.savefig(\"rmse_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Plot ranking metrics\n",
    "        ranking_metrics = metrics_df.melt(id_vars=[\"Model\"], value_vars=[\"Precision@10\", \"Recall@10\", \"NDCG@10\"],\n",
    "                                          var_name=\"Metric\", value_name=\"Value\")\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.barplot(x=\"Metric\", y=\"Value\", hue=\"Model\", data=ranking_metrics)\n",
    "        plt.title(\"Ranking Metrics Comparison Across Models\")\n",
    "        plt.savefig(\"ranking_metrics.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Plot statistical tests\n",
    "        stats_df = pd.DataFrame([\n",
    "            {\"Comparison\": k, \"t-statistic\": v[\"t\"], \"p-value\": v[\"p\"]} for k, v in stats_results.items()\n",
    "        ])\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        bars = sns.barplot(x=\"Comparison\", y=\"t-statistic\", data=stats_df)\n",
    "        for i, bar in enumerate(bars.patches):\n",
    "            p_val = stats_df.iloc[i][\"p-value\"]\n",
    "            bars.annotate(f\"p={p_val:.4f}\",\n",
    "                          (bar.get_x() + bar.get_width() / 2, bar.get_height()),\n",
    "                          ha=\"center\", va=\"bottom\", fontsize=10)\n",
    "        plt.title(\"Statistical Tests (t-statistics with p-values)\")\n",
    "        plt.xticks(rotation=45, ha=\"right\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"statistical_tests.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Plot fairness (budget tier diversity)\n",
    "        fairness_df = []\n",
    "        for model, tiers in fairness_results.items():\n",
    "            for tier, count in tiers.items():\n",
    "                fairness_df.append(\n",
    "                    {\"Model\": model, \"Budget Tier\": tier, \"Count\": count})\n",
    "        fairness_df = pd.DataFrame(fairness_df)\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.barplot(x=\"Budget Tier\", y=\"Count\", hue=\"Model\", data=fairness_df)\n",
    "        plt.title(\"Diversity by Budget Tier Across Models\")\n",
    "        plt.savefig(\"diversity_comparison.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Log visualization completion\n",
    "        logging.info(\n",
    "            \"Visualizations saved: rmse_comparison.png, ranking_metrics.png, statistical_tests.png, diversity_comparison.png\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Failed to visualize results: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "visualize_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "def parse_all_ranking_metrics(file_paths):\n",
    "    \"\"\"Parse ranking metrics for all models from provided text files.\"\"\"\n",
    "    model_metrics = {}\n",
    "    metric_names = [\"Precision@10\", \"Recall@10\", \"NDCG@10\"]\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        with open(file_path, \"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            current_model = None\n",
    "            for line in lines:\n",
    "                line = line.strip()\n",
    "                model_match = re.match(r\"^([\\w\\s]+)RMSE:\", line)\n",
    "                if model_match:\n",
    "                    current_model = model_match.group(1).strip()\n",
    "                    if current_model not in model_metrics:\n",
    "                        model_metrics[current_model] = {}\n",
    "                for metric in metric_names:\n",
    "                    if f\"{metric}:\" in line and current_model:\n",
    "                        val_match = re.search(rf\"{metric}:\\s*([\\d.]+)\", line)\n",
    "                        if val_match:\n",
    "                            model_metrics[current_model][metric] = float(\n",
    "                                val_match.group(1))\n",
    "    return model_metrics\n",
    "\n",
    "\n",
    "def plot_ranking_metrics(metrics_dict):\n",
    "    \"\"\"Create a barplot from parsed ranking metrics.\"\"\"\n",
    "    df = pd.DataFrame([\n",
    "        {\"Model\": model, \"Metric\": metric, \"Value\": value}\n",
    "        for model, metrics in metrics_dict.items()\n",
    "        for metric, value in metrics.items()\n",
    "    ])\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(data=df, x=\"Metric\", y=\"Value\", hue=\"Model\")\n",
    "    plt.title(\"Ranking Metrics Comparison Across Models\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"fixed_ranking_metrics.png\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Usage\n",
    "ranking_metrics = parse_all_ranking_metrics([\n",
    "    \"als_evaluation_results.txt\",\n",
    "    \"ncf_evaluation_results.txt\"\n",
    "])\n",
    "plot_ranking_metrics(ranking_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-04 12:47:26,133 - INFO - Spark session closed.\n",
      "2025-08-04 12:47:26,133 - INFO - Spark session closed.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Clean Up\n",
    "def cleanup():\n",
    "    try:\n",
    "        spark.stop()\n",
    "        logging.info(\"Spark session closed.\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Cleanup failed: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "\n",
    "cleanup()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
